{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0658a37",
   "metadata": {},
   "source": [
    "Reducción de la dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6e754ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Configuración de rutas ===\n",
    "TRAIN_PATH = \"../data/dataset/X_train_scaled_leonel.csv\"  \n",
    "TEST_PATH  = \"../data/dataset/X_test_scaled_leonel.csv\"   \n",
    "OUT_DIR    = \"../data/output\"                          \n",
    "TARGET_VARIANCE = 0.95                             # 95% por defecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a34192fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eaf409",
   "metadata": {},
   "source": [
    "Antes de hacer el PCA, vamos a contar los missing values de nuestras características para ver la estrategia a seguir, ya que no podemos tener missing values para PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d791e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(no genres listed)     868\n",
      "Action                 868\n",
      "Adventure              868\n",
      "Animation              868\n",
      "Children               868\n",
      "                      ... \n",
      "zernike_20            1856\n",
      "zernike_21            1856\n",
      "zernike_22            1856\n",
      "zernike_23            1856\n",
      "zernike_24            1856\n",
      "Length: 2024, dtype: int64\n",
      "Colunas con al menos 1 missing value: 2002 \n",
      "\n",
      "\n",
      "=== NA por columna (ordenado) ===\n",
      "            dtype  missing values  missing_%\n",
      "hog_1763  float64            1867      24.29\n",
      "hog_1762  float64            1867      24.29\n",
      "hog_1761  float64            1867      24.29\n",
      "hog_1760  float64            1867      24.29\n",
      "hog_1759  float64            1867      24.29\n",
      "hog_1758  float64            1867      24.29\n",
      "hog_1757  float64            1867      24.29\n",
      "hog_1756  float64            1867      24.29\n",
      "hog_1755  float64            1867      24.29\n",
      "hog_1754  float64            1867      24.29\n",
      "hog_1753  float64            1867      24.29\n",
      "hog_1752  float64            1867      24.29\n",
      "hog_1751  float64            1867      24.29\n",
      "hog_1750  float64            1867      24.29\n",
      "hog_1749  float64            1867      24.29\n",
      "hog_1748  float64            1867      24.29\n",
      "hog_1747  float64            1867      24.29\n",
      "hog_1746  float64            1867      24.29\n",
      "hog_1745  float64            1867      24.29\n",
      "hog_1744  float64            1867      24.29\n",
      "hog_1743  float64            1867      24.29\n",
      "hog_1742  float64            1867      24.29\n",
      "hog_1741  float64            1867      24.29\n",
      "hog_1740  float64            1867      24.29\n",
      "hog_1739  float64            1867      24.29\n",
      "hog_1738  float64            1867      24.29\n",
      "hog_1737  float64            1867      24.29\n",
      "hog_1736  float64            1867      24.29\n",
      "hog_1735  float64            1867      24.29\n",
      "hog_1734  float64            1867      24.29\n",
      "hog_1733  float64            1867      24.29\n",
      "hog_1732  float64            1867      24.29\n",
      "hog_1731  float64            1867      24.29\n",
      "hog_1730  float64            1867      24.29\n",
      "hog_1729  float64            1867      24.29\n",
      "hog_1728  float64            1867      24.29\n",
      "hog_1727  float64            1867      24.29\n",
      "hog_1726  float64            1867      24.29\n",
      "hog_1725  float64            1867      24.29\n",
      "hog_1724  float64            1867      24.29\n",
      "hog_1723  float64            1867      24.29\n",
      "hog_1722  float64            1867      24.29\n",
      "hog_1721  float64            1867      24.29\n",
      "hog_1720  float64            1867      24.29\n",
      "hog_1719  float64            1867      24.29\n",
      "hog_1718  float64            1867      24.29\n",
      "hog_1717  float64            1867      24.29\n",
      "hog_1716  float64            1867      24.29\n",
      "hog_1715  float64            1867      24.29\n",
      "hog_1714  float64            1867      24.29\n",
      "\n",
      "Total columnas: 2024, filas: 7687\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv(TRAIN_PATH)\n",
    "X_test  = pd.read_csv(TEST_PATH)\n",
    "\n",
    "missing_cnt = X_train.isna().sum()\n",
    "print(missing_cnt)\n",
    "missing_cols = missing_cnt[missing_cnt > 0].sort_values(ascending=False)\n",
    "print(\"Colunas con al menos 1 missing value:\",missing_cols.shape[0], \"\\n\")\n",
    "missing_pct = X_train.isna().mean() * 100\n",
    "summary = (pd.DataFrame({\n",
    "    \"dtype\": X_train.dtypes.astype(str),\n",
    "    \"missing values\": missing_cnt,\n",
    "    \"missing_%\": missing_pct.round(2)\n",
    "})\n",
    ".sort_values(\"missing_%\", ascending=False))\n",
    "\n",
    "print(\"\\n=== NA por columna (ordenado) ===\")\n",
    "print(summary.head(50))  # muestra las 50 con más NA\n",
    "print(f\"\\nTotal columnas: {X_train.shape[1]}, filas: {X_train.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e8fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas totales: 2024\n",
      "Columnas con ≥1 NaN: 2002\n",
      "\n",
      "TOP 30 columnas con más NaN (%):\n",
      "hog_1763    24.29\n",
      "hog_1762    24.29\n",
      "hog_1761    24.29\n",
      "hog_1760    24.29\n",
      "hog_1759    24.29\n",
      "hog_1758    24.29\n",
      "hog_1757    24.29\n",
      "hog_1756    24.29\n",
      "hog_1755    24.29\n",
      "hog_1754    24.29\n",
      "hog_1753    24.29\n",
      "hog_1752    24.29\n",
      "hog_1751    24.29\n",
      "hog_1750    24.29\n",
      "hog_1749    24.29\n",
      "hog_1748    24.29\n",
      "hog_1747    24.29\n",
      "hog_1746    24.29\n",
      "hog_1745    24.29\n",
      "hog_1744    24.29\n",
      "hog_1743    24.29\n",
      "hog_1742    24.29\n",
      "hog_1741    24.29\n",
      "hog_1740    24.29\n",
      "hog_1739    24.29\n",
      "hog_1738    24.29\n",
      "hog_1737    24.29\n",
      "hog_1736    24.29\n",
      "hog_1735    24.29\n",
      "hog_1734    24.29\n",
      "dtype: float64\n",
      "\n",
      "Columnas a eliminar (≥60.0% NaN): 0\n",
      "[]\n",
      "Filas con >50% NaN (en columnas conservadas): 1867\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# --- 1) Resumen de faltantes ---\n",
    "missing_cnt = X_train.isna().sum()\n",
    "missing_pct = (X_train.isna().mean() * 100).round(2)\n",
    "\n",
    "n_cols_total = X_train.shape[1]\n",
    "n_cols_with_na = (missing_cnt > 0).sum()\n",
    "print(f\"Columnas totales: {n_cols_total}\")\n",
    "print(f\"Columnas con ≥1 NaN: {n_cols_with_na}\")\n",
    "\n",
    "# Top columnas con mas nan\n",
    "top_na = missing_pct.sort_values(ascending=False).head(30)\n",
    "print(\"\\nTOP 30 columnas con más NaN (%):\")\n",
    "print(top_na)\n",
    "\n",
    "# 2) Selección de columnas por umbral\n",
    "UMBRAL_DROP = 60.0  # 60% nan\n",
    "drop_cols = missing_pct[missing_pct >= UMBRAL_DROP].index.tolist()\n",
    "keep_cols = [c for c in X_train.columns if c not in drop_cols]\n",
    "\n",
    "print(f\"\\nColumnas a eliminar (≥{UMBRAL_DROP}% NaN): {len(drop_cols)}\")\n",
    "if len(drop_cols) <= 20:\n",
    "    print(drop_cols)\n",
    "\n",
    "# descartar filas con muchísimos NaN (sobre keep_cols)\n",
    "row_pct_na = X_train[keep_cols].isna().mean(axis=1) * 100\n",
    "bad_rows = row_pct_na[row_pct_na > 50].index  # ajusta 40–50%\n",
    "print(f\"Filas con >50% NaN (en columnas conservadas): {len(bad_rows)}\")\n",
    "\n",
    "Xf = X_train.drop(columns=drop_cols).drop(index=bad_rows).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d33ce6e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Faltantes en Xf ===\n",
      "Filas x Columnas: (5820, 2024)\n",
      "Total de celdas: 11779680\n",
      "Celdas con NaN: 13839 (0.1175%)\n",
      "Columnas con ≥1 NaN: 21 / 2024\n",
      "Filas con ≥1 NaN: 659 / 5820\n",
      "\n",
      "Top 20 columnas con más NaN:\n",
      "(no genres listed)    659\n",
      "Action                659\n",
      "Adventure             659\n",
      "Animation             659\n",
      "Children              659\n",
      "Comedy                659\n",
      "Crime                 659\n",
      "Documentary           659\n",
      "Drama                 659\n",
      "Fantasy               659\n",
      "Film-Noir             659\n",
      "Horror                659\n",
      "IMAX                  659\n",
      "Musical               659\n",
      "Mystery               659\n",
      "Romance               659\n",
      "Sci-Fi                659\n",
      "Thriller              659\n",
      "War                   659\n",
      "Western               659\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "total_celdas = Xf.shape[0] * Xf.shape[1]\n",
    "\n",
    "# Por columna\n",
    "missing_cnt_cols = Xf.isna().sum()\n",
    "missing_pct_cols = (Xf.isna().mean() * 100).round(2)\n",
    "\n",
    "# Columnas con ≥1 NaN\n",
    "missing_cols = missing_cnt_cols[missing_cnt_cols > 0].sort_values(ascending=False)\n",
    "\n",
    "# Totales\n",
    "n_missing_cells = int(missing_cnt_cols.sum())\n",
    "n_cols_with_na  = int((missing_cnt_cols > 0).sum())\n",
    "n_rows_with_na  = int(Xf.isna().any(axis=1).sum())\n",
    "pct_cells_na    = round(100 * n_missing_cells / total_celdas, 4)\n",
    "\n",
    "print(\"=== Faltantes en Xf ===\")\n",
    "print(\"Filas x Columnas:\", Xf.shape)\n",
    "print(\"Total de celdas:\", total_celdas)\n",
    "print(\"Celdas con NaN:\", n_missing_cells, f\"({pct_cells_na}%)\")\n",
    "print(\"Columnas con ≥1 NaN:\", n_cols_with_na, \"/\", Xf.shape[1])\n",
    "print(\"Filas con ≥1 NaN:\", n_rows_with_na, \"/\", Xf.shape[0])\n",
    "\n",
    "# (Opcional) Top columnas con más NaN\n",
    "print(\"\\nTop 20 columnas con más NaN:\")\n",
    "print(missing_cols.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2e7512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes → train: (7687, 2024)  test: (3210, 2003)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    # Limpieza por si el índice está guardado como columna\n",
    "for df in (X_train, X_test):\n",
    "    if df.columns[0].lower() in (\"unnamed: 0\", \"index\", \"\"):\n",
    "        df.drop(df.columns[0], axis=1, inplace=True)\n",
    "    df[:] = df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "print(\"Shapes → train:\", X_train.shape, \" test:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6297938d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m pca_full = PCA()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mpca_full\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m explained_var = pca_full.explained_variance_ratio_\n\u001b[32m      4\u001b[39m cum_explained = np.cumsum(explained_var)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1466\u001b[39m     estimator._validate_params()\n\u001b[32m   1468\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1469\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1470\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1471\u001b[39m     )\n\u001b[32m   1472\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1473\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:448\u001b[39m, in \u001b[36mPCA.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    430\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    432\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X.\u001b[39;00m\n\u001b[32m    433\u001b[39m \n\u001b[32m    434\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    446\u001b[39m \u001b[33;03m        Returns the instance itself.\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:511\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    502\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPCA with svd_solver=\u001b[39m\u001b[33m'\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is not supported for Array API inputs.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    503\u001b[39m     )\n\u001b[32m    505\u001b[39m \u001b[38;5;66;03m# Validate the data, without ever forcing a copy as any solver that\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[38;5;66;03m# supports sparse input data and the `covariance_eigh` solver are\u001b[39;00m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# written in a way to avoid the need for any inplace modification of\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# the input data contrary to the other solvers.\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;66;03m# The copy will happen\u001b[39;00m\n\u001b[32m    510\u001b[39m \u001b[38;5;66;03m# later, only if needed, once the solver negotiation below is done.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[38;5;28mself\u001b[39m._fit_svd_solver = \u001b[38;5;28mself\u001b[39m.svd_solver\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver == \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[39m, in \u001b[36mBaseEstimator._validate_data\u001b[39m\u001b[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[39m\n\u001b[32m    631\u001b[39m         out = X, y\n\u001b[32m    632\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[32m--> \u001b[39m\u001b[32m633\u001b[39m     out = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[32m    635\u001b[39m     out = _check_y(y, **check_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1058\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1059\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1060\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1061\u001b[39m     )\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1065\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1066\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1067\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1068\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1069\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1072\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1073\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    121\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\laura\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    156\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    157\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    158\u001b[39m     msg_err += (\n\u001b[32m    159\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    160\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    170\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    171\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "pca_full = PCA()\n",
    "pca_full.fit(X_train.values)\n",
    "explained_var = pca_full.explained_variance_ratio_\n",
    "cum_explained = np.cumsum(explained_var)\n",
    "\n",
    "# Selección de k (mínimo con ≥ TARGET_VARIANCE)\n",
    "k = int(np.searchsorted(cum_explained, TARGET_VARIANCE) + 1)\n",
    "print(f\"Componentes totales: {len(explained_var)} | k seleccionado (≥{int(TARGET_VARIANCE*100)}%): {k}\")\n",
    "print(f\"Varianza acumulada con k={k}: {cum_explained[k-1]:.4f}\")\n",
    "\n",
    "# Tabla de varianza explicada\n",
    "var_table = pd.DataFrame({\n",
    "    'component': np.arange(1, len(explained_var)+1),\n",
    "    'explained_variance_ratio': explained_var,\n",
    "    'cumulative_variance_ratio': cum_explained\n",
    "})\n",
    "display(var_table.head(25))\n",
    "\n",
    "# Gráficos\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(explained_var)+1), explained_var, marker='o')\n",
    "plt.xlabel('Componente principal')\n",
    "plt.ylabel('Varianza explicada (ratio)')\n",
    "plt.title('Scree plot - PCA (todas las componentes)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(1, len(cum_explained)+1), cum_explained, marker='o')\n",
    "plt.axhline(y=TARGET_VARIANCE, linestyle='--')\n",
    "plt.axvline(x=k, linestyle='--')\n",
    "plt.xlabel('Número de componentes')\n",
    "plt.ylabel('Varianza explicada acumulada')\n",
    "plt.title(f'Varianza acumulada - PCA (k={k})')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729369a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_train: (6819, 19) Z_test: (2923, 19)\n"
     ]
    }
   ],
   "source": [
    "# Refit con k componentes y transformación de train/test \n",
    "pca_k = PCA(n_components=k, svd_solver='full', random_state=0)\n",
    "pca_k.fit(X_train.values)\n",
    "Z_train = pca_k.transform(X_train.values)\n",
    "Z_test  = pca_k.transform(X_test.values)\n",
    "print(\"Z_train:\", Z_train.shape, \"Z_test:\", Z_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e86166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado:\n",
      "- ..\\data\\output\\X_train_pca_k19.csv\n",
      "- ..\\data\\output\\X_test_pca_k19.csv\n",
      "- ..\\data\\output\\pca_variance_report.csv\n",
      "- ..\\data\\output\\pca_metadata.json\n",
      "- ..\\data\\output\\pca_model_k19.pkl\n"
     ]
    }
   ],
   "source": [
    "# === Guardado de resultados y artefactos ===\n",
    "out_dir = Path(OUT_DIR)\n",
    "train_out = out_dir / f\"X_train_pca_k{k}.csv\"\n",
    "test_out  = out_dir / f\"X_test_pca_k{k}.csv\"\n",
    "var_out   = out_dir / \"pca_variance_report.csv\"\n",
    "meta_out  = out_dir / \"pca_metadata.json\"\n",
    "model_out = out_dir / f\"pca_model_k{k}.pkl\"\n",
    "\n",
    "pd.DataFrame(Z_train).to_csv(train_out, index=False)\n",
    "pd.DataFrame(Z_test).to_csv(test_out, index=False)\n",
    "var_table.to_csv(var_out, index=False)\n",
    "\n",
    "meta = {\n",
    "    \"train_path\": TRAIN_PATH,\n",
    "    \"test_path\": TEST_PATH,\n",
    "    \"train_shape\": tuple(X_train.shape),\n",
    "    \"test_shape\": tuple(X_test.shape),\n",
    "    \"n_features\": int(X_train.shape[1]),\n",
    "    \"k_selected\": int(k),\n",
    "    \"target_variance\": float(TARGET_VARIANCE),\n",
    "    \"achieved_variance\": float(np.cumsum(pca_k.explained_variance_ratio_)[-1])\n",
    "}\n",
    "with open(meta_out, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "with open(model_out, \"wb\") as f:\n",
    "    pickle.dump(pca_k, f)\n",
    "\n",
    "print(\"Guardado:\")\n",
    "print(\"-\", train_out)\n",
    "print(\"-\", test_out)\n",
    "print(\"-\", var_out)\n",
    "print(\"-\", meta_out)\n",
    "print(\"-\", model_out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
